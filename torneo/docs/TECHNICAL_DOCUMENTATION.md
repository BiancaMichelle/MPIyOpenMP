# ğŸ“– DocumentaciÃ³n TÃ©cnica - Torneo One Piece MPI/OpenMP

## ğŸ—ï¸ Arquitectura del Sistema

### VisiÃ³n General
El proyecto implementa un sistema de simulaciÃ³n paralela que combina:
- **MPI (Message Passing Interface)**: Para paralelizaciÃ³n distribuida entre "universos"
- **OpenMP**: Para paralelizaciÃ³n de memoria compartida dentro de cada universo
- **Interfaz Web**: Para configuraciÃ³n y visualizaciÃ³n de resultados

### Diagrama de Arquitectura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Torneo One Piece                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Web Interface          â”‚  Core Engine (C)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Configurator    â”‚   â”‚  â”‚ MPI Process 0 (Master)      â”‚   â”‚
â”‚  â”‚ Results Viewer  â”‚   â”‚  â”‚ - Load config               â”‚   â”‚
â”‚  â”‚ Static Assets   â”‚   â”‚  â”‚ - Broadcast data            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚ - Collect results           â”‚   â”‚
â”‚           â”‚             â”‚  â”‚ - Write output              â”‚   â”‚
â”‚           â–¼             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚             â”‚                      â”‚
â”‚  â”‚ config.json     â”‚   â”‚             â–¼                      â”‚
â”‚  â”‚ resultado.json  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚ MPI Process 1..N (Workers)  â”‚   â”‚
â”‚                         â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚                         â”‚  â”‚ â”‚ OpenMP Threads         â”‚ â”‚   â”‚
â”‚                         â”‚  â”‚ â”‚ - Evaluate characters  â”‚ â”‚   â”‚
â”‚                         â”‚  â”‚ â”‚ - Calculate power      â”‚ â”‚   â”‚
â”‚                         â”‚  â”‚ â”‚ - Find local champion  â”‚ â”‚   â”‚
â”‚                         â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ ImplementaciÃ³n TÃ©cnica

### Estructura de Datos
```c
typedef struct {
    char nombre[50];
    int activo;
} Personaje;

typedef struct {
    char tripulacion[50];
    Personaje personajes[10];
    int num_personajes;
    int activo;
} Tripulacion;
```

### Flujo de EjecuciÃ³n MPI

1. **InicializaciÃ³n**
   ```c
   MPI_Init(&argc, &argv);
   MPI_Comm_rank(MPI_COMM_WORLD, &rank);
   MPI_Comm_size(MPI_COMM_WORLD, &size);
   ```

2. **DistribuciÃ³n de Datos**
   ```c
   if (rank == 0) {
       cargar_config("config/config.json");
   }
   MPI_Bcast(&total_tripulaciones, 1, MPI_INT, 0, MPI_COMM_WORLD);
   MPI_Bcast(tripulaciones, sizeof(tripulaciones), MPI_BYTE, 0, MPI_COMM_WORLD);
   ```

3. **Procesamiento Paralelo Local (OpenMP)**
   ```c
   #pragma omp parallel
   {
       unsigned int seed = time(NULL) ^ omp_get_thread_num() ^ rank;
       for (int j = 0; j < t.num_personajes; j++) {
           if (t.personajes[j].activo) {
               int poder = rand_r(&seed) % 100;
               #pragma omp critical
               {
                   if (poder > poder_local) {
                       poder_local = poder;
                       ganador_local = j;
                       strcpy(nombre_local, t.personajes[j].nombre);
                   }
               }
           }
       }
   }
   ```

4. **ReducciÃ³n Global**
   ```c
   struct {
       int poder;
       int rank;
   } in, out;
   
   in.poder = poder_local;
   in.rank = rank;
   
   MPI_Reduce(&in, &out, 1, MPI_2INT, MPI_MAXLOC, 0, MPI_COMM_WORLD);
   ```

### PatrÃ³n de ComunicaciÃ³n

```
Proceso 0 (Master)          Procesos 1..N (Workers)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Config â”‚            â”‚ Wait Data   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                          â”‚
      â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Broadcast   â”‚â”â”â”â”â”â”â”â”â”â”â”â–¶â”‚ Receive     â”‚
â”‚ Data        â”‚            â”‚ Data        â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                          â”‚
      â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Process     â”‚            â”‚ Process     â”‚
â”‚ Universe 0  â”‚            â”‚ Universe N  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                          â”‚
      â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Collect     â”‚â—€â”â”â”â”â”â”â”â”â”â”â”â”‚ Send        â”‚
â”‚ Results     â”‚            â”‚ Result      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ CaracterÃ­sticas de ParalelizaciÃ³n

### MPI - ParalelizaciÃ³n Distribuida
- **Granularidad**: Una tripulaciÃ³n por proceso
- **ComunicaciÃ³n**: 
  - `MPI_Bcast`: DistribuciÃ³n de configuraciÃ³n
  - `MPI_Reduce`: AgregaciÃ³n de resultados con `MPI_MAXLOC`
  - `MPI_Send/Recv`: Transferencia del nombre del ganador
- **Escalabilidad**: O(log n) para reducciÃ³n, O(1) para broadcast

### OpenMP - ParalelizaciÃ³n de Memoria Compartida
- **Granularidad**: Un personaje por hilo
- **Directivas utilizadas**:
  - `#pragma omp parallel`: RegiÃ³n paralela
  - `#pragma omp critical`: SecciÃ³n crÃ­tica para actualizar campeÃ³n local
- **Ventajas**: Acceso directo a memoria compartida, overhead mÃ­nimo

### SincronizaciÃ³n y Seguridad
- **Semillas aleatorias Ãºnicas**: `time(NULL) ^ omp_get_thread_num() ^ rank`
- **SecciÃ³n crÃ­tica**: Protege la actualizaciÃ³n del campeÃ³n local
- **Determinismo**: Resultados reproducibles con semillas fijas

## ğŸ“Š AnÃ¡lisis de Rendimiento

### Complejidad Temporal
- **Serial**: O(T Ã— P) donde T = tripulaciones, P = personajes
- **MPI**: O(T/N Ã— P) donde N = procesos MPI
- **MPI + OpenMP**: O(T/N Ã— P/M) donde M = hilos OpenMP

### Escalabilidad TeÃ³rica
```
Speedup = 1 / (f + (1-f)/N)  # Ley de Amdahl
```
Donde:
- f = fracciÃ³n serial (configuraciÃ³n, I/O)
- N = nÃºmero de procesadores
- En nuestro caso: f â‰ˆ 0.1 (90% paralelizable)

### MÃ©tricas de Rendimiento
- **Eficiencia MPI**: Alta (comunicaciÃ³n mÃ­nima)
- **Eficiencia OpenMP**: Media (secciÃ³n crÃ­tica pequeÃ±a)
- **Balance de carga**: Bueno (trabajo uniforme por tripulaciÃ³n)

## ğŸ› ï¸ ConfiguraciÃ³n y OptimizaciÃ³n

### Variables de Entorno Importantes
```bash
export OMP_NUM_THREADS=4        # Hilos OpenMP por proceso
export OMP_SCHEDULE=static      # PlanificaciÃ³n de hilos
export OMPI_MCA_btl_base_warn_component_unused=0  # Suprimir warnings
```

### ParÃ¡metros de CompilaciÃ³n
```makefile
CFLAGS = -fopenmp -Wall -std=c99 -O3 -march=native
```

### Optimizaciones Implementadas
1. **Semillas independientes**: Evita correlaciÃ³n entre hilos/procesos
2. **MinimizaciÃ³n de secciones crÃ­ticas**: Solo actualizaciÃ³n de mÃ¡ximo
3. **ComunicaciÃ³n colectiva eficiente**: `MPI_Reduce` vs mÃºltiples `MPI_Send`
4. **Localidad de datos**: Estructuras contiguas en memoria

## ğŸ” Debugging y Profiling

### CompilaciÃ³n Debug
```bash
make CFLAGS="-fopenmp -Wall -g -DDEBUG -fsanitize=thread"
```

### Herramientas Recomendadas
- **Intel Inspector**: DetecciÃ³n de race conditions
- **Valgrind**: AnÃ¡lisis de memoria (sin OpenMP)
- **Intel VTune**: Profiling de rendimiento
- **TAU**: AnÃ¡lisis MPI + OpenMP

### Puntos de VerificaciÃ³n
1. **Race conditions**: En secciÃ³n crÃ­tica OpenMP
2. **Deadlocks**: En comunicaciÃ³n MPI
3. **Memory leaks**: En cJSON parsing
4. **Load balancing**: DistribuciÃ³n uniforme de trabajo

## ğŸ“ˆ Casos de Uso y Extensiones

### Escalabilidad
- **MÃ­nimo**: 1 proceso MPI, 1 hilo OpenMP
- **TÃ­pico**: 4 procesos MPI, 4 hilos OpenMP
- **MÃ¡ximo**: N procesadores disponibles

### Extensiones Posibles
1. **Algoritmos adaptativos**: Balanceo dinÃ¡mico de carga
2. **Persistencia**: Base de datos para histÃ³rico
3. **VisualizaciÃ³n**: GrÃ¡ficos en tiempo real
4. **Red**: EjecuciÃ³n distribuida real
5. **GPU**: AceleraciÃ³n CUDA/OpenCL

### Variaciones del Algoritmo
```c
// VersiÃ³n con diferentes mÃ©tricas de poder
int calcular_poder_avanzado(Personaje p, int seed) {
    int base = rand_r(&seed) % 100;
    int bonus = strlen(p.nombre) * 5;  // Nombres largos = mÃ¡s poder
    return base + bonus;
}
```

## ğŸš€ Deployment y DistribuciÃ³n

### ContenedorizaciÃ³n
```dockerfile
FROM ubuntu:22.04
RUN apt-get update && apt-get install -y \
    libopenmpi-dev \
    libcjson-dev \
    python3
COPY . /app
WORKDIR /app
RUN make
EXPOSE 8000
CMD ["make", "demo"]
```

### Cluster Computing
```bash
# SLURM job script
#SBATCH --nodes=4
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --time=00:30:00

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
mpirun ./src/torneo_onepiece
```

## ğŸ“š Referencias y Recursos

### DocumentaciÃ³n Oficial
- [MPI Standard](https://www.mpi-forum.org/docs/)
- [OpenMP Specification](https://www.openmp.org/specifications/)
- [cJSON Library](https://github.com/DaveGamble/cJSON)

### Papers y Estudios
- "Parallel Programming in C with MPI and OpenMP" - Quinn
- "Using MPI: Portable Parallel Programming" - Gropp et al.
- "OpenMP: An Industry Standard API" - Dagum & Menon

### Herramientas de Desarrollo
- **Compiladores**: GCC, Intel C++, Clang
- **Debuggers**: GDB, DDT, Intel Debugger
- **Profilers**: Scalasca, Score-P, Intel VTune

---

*Esta documentaciÃ³n tÃ©cnica estÃ¡ diseÃ±ada para desarrolladores que quieran entender, modificar o extender el proyecto Torneo One Piece MPI/OpenMP.*